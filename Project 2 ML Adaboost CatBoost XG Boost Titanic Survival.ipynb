{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Boosting algorithms in Python, step by step, using Titanic Data\n",
    "\n",
    "### Project of Adaboost CatBoost XG Boost Titanic Survival ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this command in Anaconda prompt which will be in windows start menu after search you will get prompt\n",
    "\n",
    "jupyter nbconvert --clear-output --inplace \"Project 2 ML Adaboost CatBoost XG Boost Titanic Survival.ipynb\"\n",
    "\n",
    "### inside the jupyter need to run this\n",
    "\n",
    "!jupyter nbconvert --clear-output --inplace \"Project 2 ML Adaboost CatBoost XG Boost Titanic Survival.ipynb\"\n",
    "\n",
    "### inside the python terminal need to run this\n",
    "\n",
    "python -m nbconvert --clear-output --inplace \"Project 2 ML Adaboost CatBoost XG Boost Titanic Survival.ipynb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sinking of the Titanic on April 15th, 1912 is one of the most tragic tragedies in history. The Titanic sank, during her maiden voyage, after colliding with an iceberg, killing 1502 out of 2224 passengers. The numbers of survivors were low due to the lack of lifeboats for all passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others, such as women, children, and upper-class. This case study analyzes what sorts of people were likely to survive this tragedy. The dataset includes the following: \n",
    "\n",
    "- Pclass:\tTicket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    "- Sex:    Sex\t\n",
    "- Age:    Age in years\t\n",
    "- Sibsp:\t# of siblings / spouses aboard the Titanic\t\n",
    "- Parch:\t# of parents / children aboard the Titanic\t\n",
    "- Ticket:\tTicket number\t\n",
    "- Fare:\tPassenger fare\t\n",
    "- Cabin:\tCabin number\t\n",
    "- Embarked:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "\n",
    "- Target class: Survived: Survival\t(0 = No, 1 = Yes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES\n",
    "We are importing all the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to run this\n",
    "\n",
    "!pip install pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas numpy matplotlib seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data using pandas dataframe\n",
    "train = pd.read_csv('Project 2 ML titanic_train Algo Ada Cat XG Boost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data head!\n",
    "train.head()\n",
    "\n",
    "#Dependent varaible is survived and rest of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been properly imported. \n",
    "#### Observation :  We can't predict anything with the PasserngerId, Name, and Ticket column, hence we will drop it. Here, we understand Survived is our dependent variable. The place where the person has boarded the ship (Embarked column) shouldn't be predicting their chance of survival, hence it will also be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependent varaible is survived and rest of the features\n",
    "train.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Embarked\"],axis=1,inplace=True)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE/VISUALIZE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXPLORE/VISUALIZE DATASET# Let's count the number of survivors and non-survivors\n",
    "train['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :  Here 1 means survived and 0 means died. Pretty much balanced data, since number of 1 and 0 are close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Number of people travelling by different class\n",
    "plt.figure(figsize=[10,5])\n",
    "sns.countplot(x = 'Pclass', data = train)\n",
    "plt.show()\n",
    "#count plot is for each sub-category, that's why we are using this count plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :  Maximum people were travelling by 3rd class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "#Observation :  Here 1 means survived and 0 means died. \n",
    "#Pretty much balanced data, since number of 1 and 0 are close.\n",
    "sns.countplot(x = 'Pclass', hue = 'Survived', data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :  More people travelling by 1st class survived; it's almost equal for 2nd class (marginally more people die though), and most of the people travelling by 3rd class died. This chart shows that the class had some impact whether a person would survive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the same analysis will be made on other independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "sns.countplot(x = 'SibSp', hue = 'Survived', data=train)\n",
    "plt.show()\n",
    "#so we are doing comparison independent with dependent variable that is called bi-variate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :  Bar Chart to indicate the number of people survived based on their siblings status. If you have 1 siblings (SibSp = 1), you have a higher chance of survival compared to being alone (SibSp = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "sns.countplot(x = 'Parch', hue = 'Survived', data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :  Bar Chart to indicate the number of people survived based on their Parch status (how many parents onboard). If you have 1, 2, or 3 family members (Parch = 1,2), you have a higher chance of survival compared to being alone (Parch = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,5])\n",
    "sns.countplot(x = 'Sex', hue = 'Survived', data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :   Bar Chart to indicate the number of people survived based on their sex. If you are a female, you have a higher chance of survival compared to other ports!\n",
    "\n",
    "#### Female and children were given first preference for safety, hence it makes sense that gender will help to predict the chances of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.countplot(x = 'Age', hue = 'Survived', data=train)\n",
    "plt.show()\n",
    "\n",
    "#in this Age column countplot is not sufficient so we need to put histogram over this\n",
    "#so that we will see clear picture or analysis for this.\n",
    "#Histogram is always plotted with bins, but in this case we are doing countplot which \n",
    "#is very bad idea, because of overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :   Bar Chart to indicate the number of people survived based on their age. If you are a baby, you have a higher chance of survival\n",
    "\n",
    "#### Female and children were given first preference for safety, hence it makes sense that age will help to predict the chances of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age Histogram \n",
    "train['Age'].hist(bins = 40) \n",
    "#in bins give any random number(as I give hypothetical 40 number of bins)\n",
    "plt.show()\n",
    "#In histogram age column will give you intervals like age 5 to 10 to 15 to 20 to 30 to 40 like this.\n",
    "#Histogram is always plotted with bins\n",
    "\n",
    "#so when we use continous value or numerical value then we have to use histogram not a count plot\n",
    "#count plot and bar plot is for categorical not for continous or numerical\n",
    "\n",
    "#as we already checked x axis for age that people survived\n",
    "#now what is showing in y axis= (frequnecy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :   The histogram shows that majorly people were around the age of 20 to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,40))\n",
    "sns.countplot(x = 'Fare', hue = 'Survived', data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :   # Bar Chart to indicate the number of people survived based on their fare. If you pay a higher fare, you have a higher chance of survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare Histogram \n",
    "train['Fare'].hist(bins = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :   # Mostly people had paid low value fare. Only a handful of people had paid high fare. \n",
    "\n",
    "#### and done with data explonation part (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE THE DATA FOR TRAINING/ DATA CLEANING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing values by variables\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: There are missing values for Age and Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values by variables\n",
    "train.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Missing values are shown in percentage. 77% of total values in Cabin is missing, and 19.8% values of Age is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize which variables in the dataset are missing, only with x axis\n",
    "sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: Since a very high percentage of values in Cabin are missing, this variable is not going to help us in the model. We will drop it from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the Cabin column\n",
    "train.drop('Cabin',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the missing values in the data one more time! only with x axis\n",
    "sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: There are 19.8% of missing values in Age, we can't entirely drop the column, nor we can keep the missing values. We will replace them with the average. However, we can't replace all the missing values with the average of Age. It would be misleading. \n",
    "\n",
    "#### The mean of total Age could would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mean of total Age\n",
    "train.Age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation: If we check the average age for different sex (male and female), we can see they are different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's get the average age for male (~29) and female (~25)\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.boxplot(x='Sex', y='Age',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hence, we should be replacing the missing Age values for female with average age of female and  replace the missing Age values for male with average age of male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the missing values of Age for male\n",
    "train.loc[(train[\"Age\"].isnull()) & (train[\"Sex\"] == \"male\"),\"Age\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the average age for male\n",
    "train.loc[train[\"Sex\"] == \"male\",\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the missing values of Age for female\n",
    "train.loc[(train[\"Age\"].isnull()) & (train[\"Sex\"] == \"female\"),\"Age\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the average age for female\n",
    "train.loc[train[\"Sex\"] == \"female\",\"Age\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing age for male and female with average age of male and female respectively\n",
    "train.loc[(train[\"Age\"].isnull()) & (train[\"Sex\"] == \"male\"),\"Age\"] = train.loc[train[\"Sex\"] == \"male\",\"Age\"].mean()\n",
    "#or we can use this as well\n",
    "#train.loc[(train[\"Age\"].isnull()) & (train[\"Sex\"] == \"male\"),\"Age\"] = 30.72\n",
    "\n",
    "train.loc[(train[\"Age\"].isnull()) & (train[\"Sex\"] == \"female\"),\"Age\"] = train.loc[train[\"Sex\"] == \"female\",\"Age\"].mean()\n",
    "\n",
    "#or we can use this as well\n",
    "#train.loc[(train[\"Age\"].isnull()) & (train[\"Sex\"] == \"female\"),\"Age\"] = 27.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check again for missing values \n",
    "sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap=\"Blues\")\n",
    "plt.show()\n",
    "# now there are no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now there are no missing values in the dataset. We have completed the data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy variables\n",
    "### We need to create the dummy variables for all the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#and 1 will your base dummy, in your two category male and female so female is your base dummy\n",
    "#sex- male is 0 and sex- female is 1\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation :  We can see that here we have only one categorical variable, i.e. Sex. We will create the dummy variable as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(data=train, columns=['Sex'],drop_first=True)\n",
    "#and dummy creation is similer to encoding\n",
    "#because machine only understand numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data split\n",
    "### We will now split the data into dependent (y) and independent variable (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop the target coloumn before we do train test split\n",
    "X = train.drop('Survived',axis=1)\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will split the data into training (80% of the data) and rest 20% - named test, will be kept aside for later use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to run this\n",
    "!pip install scikit-learn xgboost catboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Adaptive Boosting\n",
    " Adaptive Boosting, or most commonly known AdaBoost. It is sequentially growing decision trees as weak learners and punishing the incorrectly predicted samples by assigning a larger weight to them after each round of prediction. This way, the algorithm is learning from previous mistakes. The final prediction is the weighted majority vote (or weighted median in case of regression problems). After training a classifier at any level, ada-boost assigns weight to each training item. Misclassified item is assigned higher weight so that it appears in the training subset of next classifier with higher probability. After each classifier is trained, a weight is assigned to the classifier as well based on accuracy. More accurate classifier is assigned higher weight so that it will have more impact in final outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier_ada = AdaBoostClassifier(random_state = 0)\n",
    "classifier_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  MODEL TESTING\n",
    "## Once the model is executed, we will predict the test data with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = classifier_ada.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will check the confusion matrix and accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall accuracy is 82% and precision for 0 and 1 are 87% and 73% respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cat Boosting\n",
    "CatBoost is an algorithm for gradient boosting on decision trees. It is used for search, recommendation systems, personal assistants, self-driving cars, weather prediction, and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is open-source and can be used by anyone.\n",
    "Catboost is a boosted decision tree machine learning algorithm. It works in the same way as other gradient boosted algorithms such as XGBoost but provides support out of the box for categorical variables, has a higher level of accuracy without tuning parameters and also offers GPU support to speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to run this\n",
    "\n",
    "!pip3 install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "#clf=catbclsf(hyper-parameter is iteration)\n",
    "#catboost algo is iteration process it keeps on executive till it finds the best\n",
    "#eval_metric is for accuracy and verbose is for output \n",
    "#that means give me verbose every 500 iteration\n",
    "clf = CatBoostClassifier(iterations=10000, eval_metric = 'Accuracy', verbose = 500)\n",
    "clf.fit(X_train, y_train, eval_set = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall accuracy is 87% and precision for 0 and 1 are 87% and 85% respectively. \n",
    "#### CatBoost has given us a significantly better result than AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier =XGBClassifier()\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall accuracy is 85% and precision for 0 and 1 are 79% and 77% respectively. \n",
    "#### CatBoost seems to have given us the best result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall of 1 - it shows what % of total 1 available in the data could be identified by the model.\n",
    "### Precision of 1 - it shows what % of total 1 predicted by the model actually has been correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority of case XG boost gives highest rating but in this case catBoost gives highest rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
